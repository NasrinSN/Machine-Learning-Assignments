{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student    : Nasrin Sultana Nipa\n",
    "#### Student ID : 50618423\n",
    "#### Assignment : 07\n",
    "#### Course     : CS-6443-001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning a Decision Tree\n",
    "\n",
    "In this assignment, we will use the \"Wine\" dataset for the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load up the Wine dataset using Scikit-Learn. It will be split into an 80%/20% train/test split, stratified by class to maintain the same proportion of all three classes in each partition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all, y_all = load_wine(return_X_y=True, as_frame=False)\n",
    "# Let's do a simple 80/20 Train/Test split, stratified to retain proportions for each class:\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.20, random_state=2024, shuffle=True, stratify=y_all\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define the Gini impurity metric.  Since we only care about the _mixture_ of labels in our partition, the only argument needed here is `y` (not `X`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(y):\n",
    "    \"\"\"\n",
    "    Computes and returns the GINI index of the labels in `y`\n",
    "    \"\"\"\n",
    "    y = np.asarray(y)\n",
    "    classes = np.unique(y)\n",
    "    p = 0.0\n",
    "    n_items = len(y)\n",
    "    if n_items == 0:\n",
    "        return 0\n",
    "    for label in classes:\n",
    "        p += (sum(y == label) / n_items) ** 2\n",
    "    return 1.0 - p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning a split threshold\n",
    "\n",
    "Below, we will define the `learn_threshold()` function that will find the best _binary_ partition of a single random variable $x$ such that the misclassification error of the first class represented in $y$ versus all others is minimized.\n",
    "\n",
    "The partition is based on the simple relation $x < t$ where $t$ is the threshold value that we are trying to learn.  We will try to pick thresholds that are \"between\" the neighboring values of $x$ where the class $y$ changes.  (No useful threshold will be found in areas in the range of $x$ where not class transitions are occurring.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-10\n",
    "\n",
    "\n",
    "def learn_threshold(x, y):\n",
    "    sort_idx = np.argsort(x)\n",
    "    x_sorted, y_sorted = x[sort_idx], y[sort_idx]\n",
    "    best_misc_err = 1.0\n",
    "    best_thresh = x_sorted[0]\n",
    "    for idx in range(len(x_sorted)):\n",
    "        if idx < len(x_sorted) - 1 and y_sorted[idx] == y_sorted[idx + 1]:\n",
    "            continue  # Skip runs where the labels are the same; focus on transitions\n",
    "        left_class = y_sorted[0] if idx > 0 else None\n",
    "        y_right = y[idx:]\n",
    "        misc_err = sum(y_right == left_class) / len(y_sorted)\n",
    "        if misc_err < best_misc_err:\n",
    "            best_misc_err = misc_err\n",
    "            best_thresh = (x[idx] + x[idx - 1]) / 2.0 if idx > 0 else x[idx] - EPSILON\n",
    "    return best_thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree induction\n",
    "\n",
    "Now, we will define the `tree_induction` function.  This is the _training_ routine for learning a decision tree.\n",
    "\n",
    "There are a lot of ways to represent a decision tree in practice.  We will choose to use a nested Python dictionary structure here, because it requires very little code to create it, and because Python dictionaries are fairly well optimized.\n",
    "\n",
    "Each \"Node\" is a dictionary entry that knows the following things (some of which depend on whether or not the node is a leaf or a decision node):\n",
    "\n",
    "* `is_leaf` : True if the node is a leaf, False otherwise.\n",
    "* `split_idx` : The index (variable column) this decision node will split on (or None if the node is a leaf).\n",
    "* `threshold` : The splitting threshold value (or None if the node is a leaf).\n",
    "* `label` : The most numerous label for values reaching this node.\n",
    "* `proba` : The probability that `label` is correct, given training values reaching this node.\n",
    "* `impurity` : The impurity metric for all values reaching this node.\n",
    "* `N` : The number of training values that reached this node.\n",
    "* `left` : The child node where samples for which $X_{split\\_idx} < \\textrm{threshold}$.  Think of this as the \"True\" branch since our test is `<`.\n",
    "* `right` : The child node where samples for which $X_{split\\_idx} \\geq \\textrm{threshold}$.  Think of this as the \"False\" branch, since our test is `<`.\n",
    "\n",
    "The nodes do not strictly require all of this information---`N` and `impurity` are provided in case you want to examine the tree to make sure the training is working correctly.\n",
    "\n",
    "**Split Impurity**\n",
    "\n",
    "We will designate the impurity of a proposed _split_ as the weighted sum of the impurities of the left partition at that split and the right partition.  We weight them with the number of samples that move into each partition.  The formula is:\n",
    "$$\n",
    "\\operatorname{SplitImpurity} = \\frac{N_{left}}{N} \\operatorname{Impurity}_{left} + \\frac{N_{right}}{N} \\operatorname{Impurity}_{right}\n",
    "$$\n",
    "\n",
    "From this number, we can see the change in impurity that would be caused by doing the split.  We want the split to reduce the total impurity (if it does not, do not split).\n",
    "\n",
    "$$\n",
    "\\Delta \\operatorname{Impurity} = \\operatorname{Impurity_{current}} - \\operatorname{SplitImpurity}\n",
    "$$\n",
    "and we want\n",
    "$$\n",
    "\\Delta \\operatorname{Impurity} > 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_induction(X, y, impurity_metric=gini):\n",
    "    X, y = np.asarray(X), np.asarray(y)\n",
    "    N, d = X.shape[0], X.shape[1]\n",
    "    if N == 0 or d == 0:\n",
    "        return None\n",
    "    current_impurity = impurity_metric(y)\n",
    "    current_label, vote_count = stats.mode(y)\n",
    "    current_node = {\n",
    "        \"is_leaf\": True,  # May change later\n",
    "        \"split_idx\": None,  # Only set on decision nodes\n",
    "        \"threshold\": None,  # Only set on decision nodes\n",
    "        \"label\": current_label,\n",
    "        \"proba\": vote_count / N,\n",
    "        \"impurity\": current_impurity,\n",
    "        \"N\": N,\n",
    "        \"left\": None,  # Will be set later if needed.\n",
    "        \"right\": None,  # Will be set later if needed.\n",
    "    }\n",
    "    # In case we already have only one class in this node, we can stop,\n",
    "    # since this will be a leaf node.\n",
    "    if current_impurity == 0:\n",
    "        return current_node\n",
    "    # Otherwise, find the best split by impurity metric\n",
    "    best_split_imp = 1e20  # Acts as \"infinity\" for initial setting\n",
    "    best_split_idx = 0\n",
    "    best_split_threshold = 0\n",
    "    for var_idx in range(d):\n",
    "        threshold = learn_threshold(X[:, var_idx], y) # - call learn_threshold with only the var_idx column of X\n",
    "        left_rows = X[:, var_idx] < threshold\n",
    "        right_rows = np.logical_not(left_rows)\n",
    "        N_left = left_rows.sum(dtype=int)\n",
    "        N_right = right_rows.sum(dtype=int)\n",
    "        left_impurity = impurity_metric(y[left_rows])\n",
    "        right_impurity = impurity_metric(y[right_rows])\n",
    "        split_impurity = (N_left / N) * left_impurity + (N_right / N) * right_impurity # Compute the SplitImpurity (see formula above).\n",
    "\n",
    "        # Keep track of the best split impurity\n",
    "        if split_impurity < best_split_imp:\n",
    "            best_split_imp = split_impurity\n",
    "            best_split_idx = var_idx\n",
    "            best_split_threshold = threshold\n",
    "    # Store our threshold and split variable index, and then create the \"left\"\n",
    "    # and \"right\" child nodes, but only if splitting actually improves impurity.\n",
    "    # (If it didn't, we leave this node as a leaf.)\n",
    "    if best_split_imp < current_impurity:\n",
    "        current_node[\"is_leaf\"] = False\n",
    "        current_node[\"threshold\"] = best_split_threshold\n",
    "        current_node[\"split_idx\"] = best_split_idx\n",
    "        left_rows = X[:, best_split_idx] < best_split_threshold\n",
    "        right_rows = np.logical_not(left_rows)\n",
    "        current_node[\"left\"] = tree_induction(\n",
    "            X[left_rows], y[left_rows], impurity_metric\n",
    "        )\n",
    "        current_node[\"right\"] = tree_induction(\n",
    "            X[right_rows], y[right_rows], impurity_metric\n",
    "        )\n",
    "    return current_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation of the tree induction algorithm will grow trees to completion, where all values reaching a leaf node have the same label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model\n",
    "\n",
    "Now, let's run the induction algorithms to train a tree on the Wine dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = tree_induction(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction \n",
    "\n",
    "Once the tree is trained, we can make predictions.  First, let's define a prediction function that knows how to follow our tree data structure and return the result (either just a label, or a label and the confidence probability if requested):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_predict(x, tree, predict_proba=False):\n",
    "    if tree is None:\n",
    "        raise RuntimeError(\"Untrained tree cannot make predictions.\")\n",
    "    current_node = tree\n",
    "    has_answer = False\n",
    "    label = None\n",
    "    proba = None\n",
    "    while not has_answer:\n",
    "        has_answer = current_node[\"is_leaf\"]\n",
    "        label = current_node[\"label\"]\n",
    "        proba = current_node[\"proba\"]\n",
    "        if not has_answer:\n",
    "            # DIRECTIONS - Check to see if x at the index stored in the current_node's\n",
    "            #        \"split_idx\" is less than the current_node's \"threshold\".\n",
    "            if x[current_node[\"split_idx\"]] < current_node[\"threshold\"]: # TODO - fill in conditional code here as directed above.\n",
    "                current_node = current_node[\"left\"]\n",
    "            else:\n",
    "                current_node = current_node[\"right\"]\n",
    "    return label if not predict_proba else (label, proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use that function to make a prediction for every sample (row) in our `X_test` data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [tree_predict(sample, tree) for sample in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see the overall accuracy.  Remember that \"guess rate\" for 3 classes is 33.3% (assuming equal numbers in each class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.67%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy_score(y_test, preds) * 100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Insight on accuracy:\n",
    "With an accuracy of 91.67%, it seems like the decision tree model performs quite well on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better idea of where the correct and incorrect predications are occurring, let's create a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Use seaborn to plot\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"OrRd\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's use it to plot our predictions versus the ground truth.  (The first parameter is the ground truth, the second is the predictions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"correct\" predictions are on the main diagonal.  Anything off-diagonal indicates a mistake, where the model predicted a label different from the actual label for that sample.  We want larger numbers on the diagonal, and mostly zeros off-diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Insights on Confusion Matrix Result:\n",
    "This is a confusion matrix for a multi-class classification problem with three classes. Each row corresponds to the actual class, while each column corresponds to the predicted class.\n",
    "\n",
    "Based on the values of the matrix:\n",
    "\n",
    "- The first row indicates that for class 0, there are 12 correct predictions, and no samples were incorrectly classified as class 1 or class 2.\n",
    "- The second row indicates that for class 1, there are 13 correct predictions, but there's one sample that was incorrectly classified as class 2.\n",
    "- The third row indicates that for class 2, there are 8 correct predictions, but there are two samples that were incorrectly classified as class 1.\n",
    "\n",
    "Overall, it seems like the model performs well, with most of the samples correctly classified. However, there are a few misclassifications, especially between classes 1 and 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different impurity metric\n",
    "Implement a different impurity metric (either [Entropy](https://www.machinelearningnuggets.com/splitting-criteria-in-decision-trees/) or [Misclassification Error](https://towardsai.net/p/l/decision-tree-splitting-entropy-vs-misclassification-error)), then train a new tree using that metric (still on the Wine training set).  Evaluate on the Wine testing set and compare the results to what you found above.  Did it make any difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using Entropy impurity: 94.44%\n"
     ]
    }
   ],
   "source": [
    "# Define the entropy function\n",
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    Computes and returns the entropy of the labels in `y`\n",
    "    \"\"\"\n",
    "    y = np.asarray(y)\n",
    "    classes = np.unique(y)\n",
    "    entropy = 0.0\n",
    "    n_items = len(y)\n",
    "    if n_items == 0:\n",
    "        return 0\n",
    "    for label in classes:\n",
    "        p = np.mean(y == label)\n",
    "        if p > 0:\n",
    "            entropy -= p * np.log2(p)\n",
    "    return entropy\n",
    "\n",
    "# Train a new decision tree using Entropy impurity metric\n",
    "tree_entropy = tree_induction(X_train, y_train, impurity_metric=entropy)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "preds_entropy = [tree_predict(sample, tree_entropy) for sample in X_test]\n",
    "\n",
    "# Compute and print the accuracy\n",
    "accuracy_entropy = accuracy_score(y_test, preds_entropy)\n",
    "print(f\"Accuracy using Entropy impurity: {accuracy_entropy * 100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the decision tree trained using the Entropy impurity metric achieved an accuracy of 94.44% on the testing set. This is slightly higher than the accuracy achieved using the Gini impurity metric (91.67%).\n",
    "\n",
    "This difference could be due to the inherent characteristics of the dataset and how each impurity metric handles the splitting criteria. In some cases, one impurity metric may perform better than the other depending on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a purity threshold\n",
    "Instead of growing the tree until all leaf nodes are pure, implement a purity threshold $\\Theta$ such that if the impurity of a node is less than $\\Theta$, the node will remain a leaf node without trying to split.\n",
    "\n",
    "To do this, copy the original `tree_induction()` function to a new cell below and name it `tree_induction_2()`.  Add `theta` as a parameter to the new function, with a default value of 0.05.  Update the body of the function to make use of `theta`.\n",
    "\n",
    "Evaluate this new version on the Wine testing set and compare the results to the original version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - add code here ot perform the instructions above.\n",
    "def tree_induction_2(X, y, impurity_metric=gini, theta=0.05):\n",
    "    X, y = np.asarray(X), np.asarray(y)\n",
    "    N, d = X.shape[0], X.shape[1]\n",
    "    if N == 0 or d == 0:\n",
    "        return None\n",
    "    current_impurity = impurity_metric(y)\n",
    "    current_label, vote_count = stats.mode(y)\n",
    "    current_node = {\n",
    "        \"is_leaf\": True,  # May change later\n",
    "        \"split_idx\": None,  # Only set on decision nodes\n",
    "        \"threshold\": None,  # Only set on decision nodes\n",
    "        \"label\": current_label,\n",
    "        \"proba\": vote_count / N,\n",
    "        \"impurity\": current_impurity,\n",
    "        \"N\": N,\n",
    "        \"left\": None,  # Will be set later if needed.\n",
    "        \"right\": None,  # Will be set later if needed.\n",
    "    }\n",
    "    # In case we already have only one class in this node, we can stop,\n",
    "    # since this will be a leaf node.\n",
    "    if current_impurity == 0 or current_impurity <= theta:\n",
    "        return current_node\n",
    "    # Otherwise, find the best split by impurity metric\n",
    "    best_split_imp = float('inf')\n",
    "    best_split_idx = 0\n",
    "    best_split_threshold = 0\n",
    "    for var_idx in range(d):\n",
    "        threshold = learn_threshold(X[:, var_idx], y)\n",
    "        left_rows = X[:, var_idx] < threshold\n",
    "        right_rows = np.logical_not(left_rows)\n",
    "        N_left = left_rows.sum(dtype=int)\n",
    "        N_right = right_rows.sum(dtype=int)\n",
    "        left_impurity = impurity_metric(y[left_rows])\n",
    "        right_impurity = impurity_metric(y[right_rows])\n",
    "        split_impurity = (N_left / N) * left_impurity + (N_right / N) * right_impurity\n",
    "        # Keep track of the best split impurity\n",
    "        if split_impurity < best_split_imp:\n",
    "            best_split_imp = split_impurity\n",
    "            best_split_idx = var_idx\n",
    "            best_split_threshold = threshold\n",
    "    # Store our threshold and split variable index, and then create the \"left\"\n",
    "    # and \"right\" child nodes, but only if splitting actually improves impurity.\n",
    "    # (If it didn't, we leave this node as a leaf.)\n",
    "    if best_split_imp < current_impurity:\n",
    "        current_node[\"is_leaf\"] = False\n",
    "        current_node[\"threshold\"] = best_split_threshold\n",
    "        current_node[\"split_idx\"] = best_split_idx\n",
    "        left_rows = X[:, best_split_idx] < best_split_threshold\n",
    "        right_rows = np.logical_not(left_rows)\n",
    "        current_node[\"left\"] = tree_induction_2(\n",
    "            X[left_rows], y[left_rows], impurity_metric, theta\n",
    "        )\n",
    "        current_node[\"right\"] = tree_induction_2(\n",
    "            X[right_rows], y[right_rows], impurity_metric, theta\n",
    "        )\n",
    "    return current_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's train a decision tree using the original tree_induction() function on the Wine training set. After training, we'll evaluate the model on the Wine testing set to get its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the original decision tree: 91.67%\n"
     ]
    }
   ],
   "source": [
    "# Train a decision tree using the original tree_induction() function on the Wine training set\n",
    "tree_original = tree_induction(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set using the original tree\n",
    "preds_original = [tree_predict(sample, tree_original) for sample in X_test]\n",
    "\n",
    "# Compute and print the accuracy of the original decision tree\n",
    "accuracy_original = accuracy_score(y_test, preds_original)\n",
    "print(f\"Accuracy of the original decision tree: {accuracy_original * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the code to train the decision tree using tree_induction_2():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the decision tree with purity threshold: 94.44%\n"
     ]
    }
   ],
   "source": [
    "# Train a decision tree using the new tree_induction_2() function on the Wine training set with a purity threshold\n",
    "tree_threshold = tree_induction_2(X_train, y_train, theta=0.05)\n",
    "\n",
    "# Make predictions on the testing set using the tree with purity threshold\n",
    "preds_threshold = [tree_predict(sample, tree_threshold) for sample in X_test]\n",
    "\n",
    "# Compute and print the accuracy of the decision tree with purity threshold\n",
    "accuracy_threshold = accuracy_score(y_test, preds_threshold)\n",
    "print(f\"Accuracy of the decision tree with purity threshold: {accuracy_threshold * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that even with the purity threshold implemented, the accuracy remains the same at 94.44%.\n",
    "\n",
    "This suggests that in this particular case, introducing a purity threshold did not significantly impact the performance of the decision tree model on the Wine testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************************************************************************************************************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Decision_tree_induction",
   "language": "python",
   "name": "decision_tree_induction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
